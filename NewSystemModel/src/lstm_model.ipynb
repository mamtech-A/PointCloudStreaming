{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Bandwidth Prediction Model\n",
    "\n",
    "This notebook implements a PyTorch LSTM model for bandwidth prediction with:\n",
    "- Train/test split for proper validation\n",
    "- Hyperparameter tuning capabilities\n",
    "- Model saving and loading functionality\n",
    "- Real-time prediction support\n",
    "\n",
    "**Author:** PointCloudStreaming Team  \n",
    "**Purpose:** Predict network bandwidth for adaptive point cloud streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We import the necessary libraries for:\n",
    "- **PyTorch**: Deep learning framework for LSTM implementation\n",
    "- **NumPy**: Numerical computing\n",
    "- **Sklearn**: Train/test split functionality\n",
    "- **Collections**: deque for efficient history management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import deque\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Constants\n",
    "\n",
    "Define constants used throughout the model training:\n",
    "- **GRADIENT_CLIP_MAX_NORM**: Maximum norm for gradient clipping to prevent exploding gradients\n",
    "- **MAPE_ZERO_THRESHOLD**: Threshold to filter near-zero values when calculating MAPE (Mean Absolute Percentage Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for model training\n",
    "GRADIENT_CLIP_MAX_NORM = 1.0  # Max norm for gradient clipping\n",
    "MAPE_ZERO_THRESHOLD = 0.1     # Threshold (Mbps) to filter near-zero values for MAPE calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BandwidthDataset Class\n",
    "\n",
    "A custom PyTorch Dataset class for handling bandwidth sequences.\n",
    "\n",
    "This class:\n",
    "- Converts input data to PyTorch FloatTensors\n",
    "- Provides standard Dataset interface (`__len__`, `__getitem__`)\n",
    "- Enables efficient batch loading with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandwidthDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for bandwidth sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BandwidthLSTM Model Architecture\n",
    "\n",
    "The core LSTM neural network architecture for bandwidth prediction.\n",
    "\n",
    "### Architecture Overview:\n",
    "1. **Input Layer**: Accepts sequences of bandwidth measurements\n",
    "2. **LSTM Layers**: Configurable number of stacked LSTM layers with hidden units\n",
    "3. **Dropout**: Regularization between LSTM layers\n",
    "4. **Fully Connected Layers**: Output layer with ReLU activation\n",
    "\n",
    "### Key Parameters:\n",
    "- `input_size`: Features per timestep (1 for univariate bandwidth)\n",
    "- `hidden_size`: Number of hidden units in LSTM\n",
    "- `num_layers`: Number of stacked LSTM layers\n",
    "- `dropout`: Dropout rate for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandwidthLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for bandwidth prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: sequence_length features\n",
    "    - LSTM layers with configurable hidden_size and num_layers\n",
    "    - Dropout for regularization\n",
    "    - Fully connected output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of features per timestep (1 for univariate bandwidth)\n",
    "            hidden_size: Number of hidden units in LSTM layers\n",
    "            num_layers: Number of stacked LSTM layers\n",
    "            dropout: Dropout rate for regularization (applied between LSTM layers)\n",
    "        \"\"\"\n",
    "        super(BandwidthLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for output\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Reshape input if needed: (batch, seq_len) -> (batch, seq_len, 1)\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use the last hidden state for prediction\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(last_hidden)\n",
    "        \n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTMPredictor Class\n",
    "\n",
    "A high-level wrapper class that provides a complete interface for:\n",
    "- **Training**: With train/test split and early stopping\n",
    "- **Hyperparameter tuning**: Grid search over parameter space\n",
    "- **Model persistence**: Save/load functionality\n",
    "- **Real-time prediction**: Maintain history and predict next value\n",
    "\n",
    "### Key Features:\n",
    "- Automatic device selection (CUDA/CPU)\n",
    "- Data normalization for numerical stability\n",
    "- Early stopping to prevent overfitting\n",
    "- Comprehensive metrics (MAE, MAPE, RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor:\n",
    "    \"\"\"\n",
    "    LSTM-based bandwidth predictor with training and inference capabilities.\n",
    "    \n",
    "    This class wraps the BandwidthLSTM model and provides methods for:\n",
    "    - Training with train/test split\n",
    "    - Hyperparameter tuning\n",
    "    - Model saving and loading\n",
    "    - Real-time prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=10, hidden_size=64, num_layers=2, \n",
    "                 dropout=0.2, learning_rate=0.001, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM predictor.\n",
    "        \n",
    "        Args:\n",
    "            sequence_length: Number of historical samples for prediction\n",
    "            hidden_size: Number of hidden units in LSTM\n",
    "            num_layers: Number of stacked LSTM layers\n",
    "            dropout: Dropout rate for regularization\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            device: Computation device ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = BandwidthLSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Statistics for normalization\n",
    "        self.stats = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "            'min': 0,\n",
    "            'max': 100\n",
    "        }\n",
    "        \n",
    "        # History for real-time prediction\n",
    "        self.history = deque(maxlen=sequence_length)\n",
    "        self.trained = False\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'test_loss': [],\n",
    "            'best_epoch': 0,\n",
    "            'best_test_loss': float('inf')\n",
    "        }\n",
    "    \n",
    "    def _normalize(self, data):\n",
    "        \"\"\"Normalize data using stored statistics.\"\"\"\n",
    "        return (data - self.stats['mean']) / (self.stats['std'] + 1e-8)\n",
    "    \n",
    "    def _denormalize(self, data):\n",
    "        \"\"\"Denormalize data using stored statistics.\"\"\"\n",
    "        return data * (self.stats['std'] + 1e-8) + self.stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Method (fit)\n",
    "\n",
    "The `fit` method trains the LSTM model with the following features:\n",
    "\n",
    "### Training Process:\n",
    "1. **Data Normalization**: Compute mean/std and normalize input data\n",
    "2. **Train/Test Split**: Split data for validation (default 80/20)\n",
    "3. **Batch Training**: Use DataLoader for efficient batch processing\n",
    "4. **Early Stopping**: Stop training if no improvement for `patience` epochs\n",
    "5. **Best Model Selection**: Save and restore the best performing model\n",
    "\n",
    "### Metrics Computed:\n",
    "- **MAE**: Mean Absolute Error (Mbps)\n",
    "- **MAPE**: Mean Absolute Percentage Error (%)\n",
    "- **RMSE**: Root Mean Square Error (Mbps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, test_size=0.2, epochs=100, batch_size=32, \n",
    "        patience=15, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the LSTM model with train/test split.\n",
    "    \n",
    "    Args:\n",
    "        X: Input sequences of shape (n_samples, sequence_length)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        test_size: Fraction of data to use for testing\n",
    "        epochs: Maximum number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        patience: Early stopping patience (epochs without improvement)\n",
    "        verbose: Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing training history and metrics\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Compute normalization statistics from training data\n",
    "    all_values = np.concatenate([X.flatten(), y])\n",
    "    self.stats['mean'] = float(np.mean(all_values))\n",
    "    self.stats['std'] = float(np.std(all_values)) if np.std(all_values) > 0 else 1.0\n",
    "    self.stats['min'] = float(np.min(all_values))\n",
    "    self.stats['max'] = float(np.max(all_values))\n",
    "    \n",
    "    # Normalize data\n",
    "    X_norm = self._normalize(X)\n",
    "    y_norm = self._normalize(y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_norm, y_norm, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset split: {len(X_train)} train, {len(X_test)} test samples\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = BandwidthDataset(X_train, y_train)\n",
    "    test_dataset = BandwidthDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_model_state = None\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    self.training_history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'best_epoch': 0,\n",
    "        'best_test_loss': float('inf')\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        self.model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(self.device)\n",
    "            batch_y = batch_y.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=GRADIENT_CLIP_MAX_NORM)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        self.model.eval()\n",
    "        test_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                test_losses.append(loss.item())\n",
    "        \n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "        \n",
    "        # Record history\n",
    "        self.training_history['train_loss'].append(avg_train_loss)\n",
    "        self.training_history['test_loss'].append(avg_test_loss)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "            self.training_history['best_epoch'] = epoch\n",
    "            self.training_history['best_test_loss'] = best_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}, \"\n",
    "                  f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        self.model.load_state_dict(best_model_state)\n",
    "        if verbose:\n",
    "            print(f\"Loaded best model from epoch {self.training_history['best_epoch']+1} \"\n",
    "                  f\"with test loss: {best_test_loss:.6f}\")\n",
    "    \n",
    "    self.trained = True\n",
    "    \n",
    "    # Compute final metrics on test set\n",
    "    test_predictions = self.predict(X_test)\n",
    "    y_test_denorm = self._denormalize(y_test)\n",
    "    test_predictions_denorm = self._denormalize(test_predictions)\n",
    "    \n",
    "    mae = np.mean(np.abs(y_test_denorm - test_predictions_denorm))\n",
    "    # Filter out near-zero values for MAPE calculation to avoid division issues\n",
    "    nonzero_mask = np.abs(y_test_denorm) > MAPE_ZERO_THRESHOLD\n",
    "    if np.sum(nonzero_mask) > 0:\n",
    "        mape = np.mean(np.abs((y_test_denorm[nonzero_mask] - test_predictions_denorm[nonzero_mask]) / y_test_denorm[nonzero_mask])) * 100\n",
    "    else:\n",
    "        mape = 0.0\n",
    "    rmse = np.sqrt(np.mean((y_test_denorm - test_predictions_denorm) ** 2))\n",
    "    \n",
    "    metrics = {\n",
    "        'mae': float(mae),\n",
    "        'mape': float(mape),\n",
    "        'rmse': float(rmse),\n",
    "        'best_epoch': self.training_history['best_epoch'],\n",
    "        'best_test_loss': float(best_test_loss)\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal Test Metrics:\")\n",
    "        print(f\"  MAE: {mae:.4f} Mbps\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        print(f\"  RMSE: {rmse:.4f} Mbps\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Add method to LSTMPredictor class\n",
    "LSTMPredictor.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Methods\n",
    "\n",
    "Methods for making predictions with the trained model:\n",
    "\n",
    "- **predict()**: Batch prediction on input sequences\n",
    "- **update()**: Add new measurement to history\n",
    "- **predict_next()**: Predict next bandwidth based on current history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    Predict bandwidth for input sequences.\n",
    "    \n",
    "    Args:\n",
    "        X: Input sequences (normalized or raw)\n",
    "        \n",
    "    Returns:\n",
    "        Predicted bandwidth values\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(X, list):\n",
    "        X = np.array(X)\n",
    "    \n",
    "    # Handle single sequence\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = self.model(X_tensor)\n",
    "    \n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "def update(self, bandwidth_mbps):\n",
    "    \"\"\"\n",
    "    Update history with a new bandwidth measurement.\n",
    "    \n",
    "    Args:\n",
    "        bandwidth_mbps: New bandwidth measurement in Mbps\n",
    "    \"\"\"\n",
    "    self.history.append(bandwidth_mbps)\n",
    "\n",
    "def predict_next(self):\n",
    "    \"\"\"\n",
    "    Predict next bandwidth value based on current history.\n",
    "    \n",
    "    Returns:\n",
    "        Predicted bandwidth in Mbps\n",
    "    \"\"\"\n",
    "    if len(self.history) < self.sequence_length:\n",
    "        # Not enough history, return average\n",
    "        return np.mean(list(self.history)) if len(self.history) > 0 else self.stats['mean']\n",
    "    \n",
    "    # Convert history to normalized sequence\n",
    "    seq = np.array(list(self.history))\n",
    "    seq_norm = self._normalize(seq)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_norm = self.predict(seq_norm)\n",
    "    \n",
    "    # Denormalize\n",
    "    pred = self._denormalize(pred_norm)\n",
    "    \n",
    "    # Clamp to reasonable range\n",
    "    pred = np.clip(pred, self.stats['min'], self.stats['max'])\n",
    "    \n",
    "    return float(pred[0]) if isinstance(pred, np.ndarray) else float(pred)\n",
    "\n",
    "# Add methods to LSTMPredictor class\n",
    "LSTMPredictor.predict = predict\n",
    "LSTMPredictor.update = update\n",
    "LSTMPredictor.predict_next = predict_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Persistence (Save/Load)\n",
    "\n",
    "Methods for saving and loading the trained model:\n",
    "\n",
    "- **save()**: Save model state, hyperparameters, and statistics to pickle file\n",
    "- **load()**: Load model from pickle file and reinitialize architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self, path):\n",
    "    \"\"\"\n",
    "    Save model to file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to save the model (.pkl file)\n",
    "    \"\"\"\n",
    "    # Ensure directory exists if path has a directory component\n",
    "    dir_path = os.path.dirname(path)\n",
    "    if dir_path:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': self.model.state_dict(),\n",
    "        'sequence_length': self.sequence_length,\n",
    "        'hidden_size': self.hidden_size,\n",
    "        'num_layers': self.num_layers,\n",
    "        'dropout': self.dropout,\n",
    "        'learning_rate': self.learning_rate,\n",
    "        'stats': self.stats,\n",
    "        'trained': self.trained,\n",
    "        'training_history': self.training_history\n",
    "    }\n",
    "    \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def load(self, path):\n",
    "    \"\"\"\n",
    "    Load model from file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the saved model (.pkl file)\n",
    "        \n",
    "    Returns:\n",
    "        Self for method chaining\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        save_dict = pickle.load(f)\n",
    "    \n",
    "    self.sequence_length = save_dict['sequence_length']\n",
    "    self.hidden_size = save_dict['hidden_size']\n",
    "    self.num_layers = save_dict['num_layers']\n",
    "    self.dropout = save_dict['dropout']\n",
    "    self.learning_rate = save_dict['learning_rate']\n",
    "    self.stats = save_dict['stats']\n",
    "    self.trained = save_dict['trained']\n",
    "    self.training_history = save_dict.get('training_history', {})\n",
    "    \n",
    "    # Reinitialize model with correct architecture\n",
    "    self.model = BandwidthLSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=self.hidden_size,\n",
    "        num_layers=self.num_layers,\n",
    "        dropout=self.dropout\n",
    "    ).to(self.device)\n",
    "    \n",
    "    # Load state dict\n",
    "    self.model.load_state_dict(save_dict['model_state_dict'])\n",
    "    \n",
    "    # Reinitialize history\n",
    "    self.history = deque(maxlen=self.sequence_length)\n",
    "    \n",
    "    return self\n",
    "\n",
    "# Add methods to LSTMPredictor class\n",
    "LSTMPredictor.save = save\n",
    "LSTMPredictor.load = load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Loading Utilities\n",
    "\n",
    "Helper functions for loading and preparing bandwidth data:\n",
    "\n",
    "- **load_bandwidth_trace()**: Parse bandwidth log files\n",
    "- **prepare_dataset()**: Create training sequences from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bandwidth_trace(log_path):\n",
    "    \"\"\"\n",
    "    Load bandwidth trace from a log file.\n",
    "    \n",
    "    Format: timestamp ms_since_start lat lon bytes_received ms_interval\n",
    "    Returns: list of bandwidth values in bps\n",
    "    \"\"\"\n",
    "    bandwidths = []\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            bytes_received = int(parts[4])\n",
    "            ms_interval = int(parts[5])\n",
    "            if ms_interval == 0:\n",
    "                continue\n",
    "            # Calculate throughput in bps\n",
    "            bps = (bytes_received * 8) / (ms_interval / 1000.0)\n",
    "            bandwidths.append(bps)\n",
    "    return bandwidths\n",
    "\n",
    "\n",
    "def prepare_dataset(bandwidth_dir, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare dataset from all bandwidth log files.\n",
    "    \n",
    "    Args:\n",
    "        bandwidth_dir: Directory containing bandwidth log files\n",
    "        sequence_length: Number of historical samples to use\n",
    "        \n",
    "    Returns:\n",
    "        X: Input sequences (historical bandwidth values in Mbps)\n",
    "        y: Target values (next bandwidth value in Mbps)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Get all log files\n",
    "    log_files = [f for f in os.listdir(bandwidth_dir) if f.endswith('.log')]\n",
    "    \n",
    "    for log_file in log_files:\n",
    "        log_path = os.path.join(bandwidth_dir, log_file)\n",
    "        bandwidths = load_bandwidth_trace(log_path)\n",
    "        \n",
    "        # Normalize to Mbps for better numerical stability\n",
    "        bandwidths_mbps = [bw / 1e6 for bw in bandwidths]\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(bandwidths_mbps) - sequence_length):\n",
    "            X.append(bandwidths_mbps[i:i + sequence_length])\n",
    "            y.append(bandwidths_mbps[i + sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "Grid search function to find optimal hyperparameters:\n",
    "\n",
    "### Search Space:\n",
    "- `hidden_size`: [32, 64, 128]\n",
    "- `num_layers`: [1, 2]\n",
    "- `dropout`: [0.1, 0.2, 0.3]\n",
    "- `learning_rate`: [0.001, 0.0005]\n",
    "\n",
    "### Selection Criterion:\n",
    "Best model is selected based on lowest MAE on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, param_grid, test_size=0.2, epochs=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using grid search.\n",
    "    \n",
    "    Args:\n",
    "        X: Input sequences\n",
    "        y: Target values\n",
    "        param_grid: Dictionary of hyperparameters to search\n",
    "        test_size: Fraction of data for testing\n",
    "        epochs: Training epochs per configuration\n",
    "        verbose: Print progress\n",
    "        \n",
    "    Returns:\n",
    "        best_params: Best hyperparameter combination\n",
    "        best_metrics: Metrics for best model\n",
    "        all_results: All hyperparameter combinations and their results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    best_params = None\n",
    "    best_metrics = None\n",
    "    best_mae = float('inf')\n",
    "    \n",
    "    # Generate all combinations\n",
    "    from itertools import product\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    combinations = list(product(*param_values))\n",
    "    total_combinations = len(combinations)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTesting {total_combinations} hyperparameter combinations...\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    for idx, combo in enumerate(combinations):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[{idx+1}/{total_combinations}] Testing: {params}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        predictor = LSTMPredictor(\n",
    "            sequence_length=params.get('sequence_length', 10),\n",
    "            hidden_size=params.get('hidden_size', 64),\n",
    "            num_layers=params.get('num_layers', 2),\n",
    "            dropout=params.get('dropout', 0.2),\n",
    "            learning_rate=params.get('learning_rate', 0.001)\n",
    "        )\n",
    "        \n",
    "        metrics = predictor.fit(\n",
    "            X, y, \n",
    "            test_size=test_size, \n",
    "            epochs=epochs, \n",
    "            batch_size=params.get('batch_size', 32),\n",
    "            patience=10,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'params': params,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  MAE: {metrics['mae']:.4f}, MAPE: {metrics['mape']:.2f}%, RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if metrics['mae'] < best_mae:\n",
    "            best_mae = metrics['mae']\n",
    "            best_params = params\n",
    "            best_metrics = metrics\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Best hyperparameters: {best_params}\")\n",
    "        print(f\"Best MAE: {best_mae:.4f} Mbps\")\n",
    "    \n",
    "    return best_params, best_metrics, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Pipeline\n",
    "\n",
    "Main training function that orchestrates the complete workflow:\n",
    "\n",
    "1. Load bandwidth traces from directory\n",
    "2. Prepare dataset with sequences\n",
    "3. Optionally perform hyperparameter tuning\n",
    "4. Train final model\n",
    "5. Save model and tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(bandwidth_dir, output_path, sequence_length=10, tune=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Train LSTM model on bandwidth traces with optional hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        bandwidth_dir: Directory containing bandwidth log files\n",
    "        output_path: Path to save trained model\n",
    "        sequence_length: Number of historical samples to use\n",
    "        tune: Whether to perform hyperparameter tuning\n",
    "        verbose: Print progress\n",
    "        \n",
    "    Returns:\n",
    "        Trained predictor\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Loading bandwidth traces from {bandwidth_dir}...\")\n",
    "    \n",
    "    X, y = prepare_dataset(bandwidth_dir, sequence_length)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Dataset size: {len(X)} sequences\")\n",
    "        print(f\"Bandwidth range: {y.min():.2f} - {y.max():.2f} Mbps\")\n",
    "        print(f\"Mean bandwidth: {y.mean():.2f} Mbps, Std: {y.std():.2f} Mbps\")\n",
    "    \n",
    "    if tune:\n",
    "        # Define hyperparameter search space\n",
    "        param_grid = {\n",
    "            'hidden_size': [32, 64, 128],\n",
    "            'num_layers': [1, 2],\n",
    "            'dropout': [0.1, 0.2, 0.3],\n",
    "            'learning_rate': [0.001, 0.0005]\n",
    "        }\n",
    "        \n",
    "        # Perform hyperparameter tuning\n",
    "        best_params, best_metrics, all_results = tune_hyperparameters(\n",
    "            X, y, param_grid, \n",
    "            test_size=0.2, \n",
    "            epochs=50, \n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Save tuning results\n",
    "        results_path = output_path.replace('.pkl', '_tuning_results.json')\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'best_params': best_params,\n",
    "                'best_metrics': best_metrics,\n",
    "                'all_results': all_results\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTuning results saved to: {results_path}\")\n",
    "            print(f\"\\nRetraining final model with best hyperparameters...\")\n",
    "        \n",
    "        # Train final model with best params\n",
    "        predictor = LSTMPredictor(\n",
    "            sequence_length=sequence_length,\n",
    "            hidden_size=best_params['hidden_size'],\n",
    "            num_layers=best_params['num_layers'],\n",
    "            dropout=best_params['dropout'],\n",
    "            learning_rate=best_params['learning_rate']\n",
    "        )\n",
    "    else:\n",
    "        # Use default hyperparameters\n",
    "        predictor = LSTMPredictor(\n",
    "            sequence_length=sequence_length,\n",
    "            hidden_size=64,\n",
    "            num_layers=2,\n",
    "            dropout=0.2,\n",
    "            learning_rate=0.001\n",
    "        )\n",
    "    \n",
    "    # Train model\n",
    "    if verbose:\n",
    "        print(\"\\nTraining final model...\")\n",
    "    \n",
    "    metrics = predictor.fit(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        patience=15,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    if verbose:\n",
    "        print(f\"\\nSaving model to {output_path}...\")\n",
    "    \n",
    "    predictor.save(output_path)\n",
    "    \n",
    "    # Also save as best model\n",
    "    best_model_path = output_path.replace('.pkl', '_best.pkl')\n",
    "    predictor.save(best_model_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Best model saved to: {best_model_path}\")\n",
    "    \n",
    "    # Test prediction on a sample\n",
    "    if verbose:\n",
    "        print(f\"\\nSample predictions:\")\n",
    "        test_sequences = X[:5]\n",
    "        test_actual = y[:5]\n",
    "        \n",
    "        for i in range(min(5, len(test_sequences))):\n",
    "            seq_norm = predictor._normalize(test_sequences[i])\n",
    "            pred_norm = predictor.predict(seq_norm)\n",
    "            pred = predictor._denormalize(pred_norm)\n",
    "            print(f\"  Actual: {test_actual[i]:.2f} Mbps, Predicted: {pred[0]:.2f} Mbps\")\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SimpleLSTM - Backward Compatibility Wrapper\n",
    "\n",
    "A wrapper class that provides backward compatibility with existing code that uses the older SimpleLSTM interface.\n",
    "\n",
    "This allows seamless migration to the new LSTM model while maintaining the old API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for LSTMPredictor.\n",
    "    \n",
    "    This allows existing code using SimpleLSTM to work with the new LSTM model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=10, alpha=0.3):\n",
    "        \"\"\"Initialize with backward-compatible interface.\"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.alpha = alpha  # Not used, kept for compatibility\n",
    "        self._predictor = None\n",
    "        self.history = deque(maxlen=sequence_length)\n",
    "        self.trained = False\n",
    "        self.stats = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "            'min': 0,\n",
    "            'max': 100\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        self._predictor = LSTMPredictor(\n",
    "            sequence_length=self.sequence_length,\n",
    "            hidden_size=64,\n",
    "            num_layers=2,\n",
    "            dropout=0.2,\n",
    "            learning_rate=0.001\n",
    "        )\n",
    "        \n",
    "        self._predictor.fit(X, y, test_size=0.2, epochs=50, verbose=False)\n",
    "        self.stats = self._predictor.stats.copy()\n",
    "        self.trained = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict bandwidth.\"\"\"\n",
    "        if self._predictor is None or not self.trained:\n",
    "            # Fallback to last value\n",
    "            if isinstance(X, np.ndarray):\n",
    "                if len(X.shape) == 1:\n",
    "                    return X[-1]\n",
    "                return np.array([seq[-1] for seq in X])\n",
    "            return X[-1]\n",
    "        \n",
    "        X_norm = self._predictor._normalize(X)\n",
    "        pred_norm = self._predictor.predict(X_norm)\n",
    "        return self._predictor._denormalize(pred_norm)\n",
    "    \n",
    "    def update(self, bandwidth_mbps):\n",
    "        \"\"\"Update history.\"\"\"\n",
    "        self.history.append(bandwidth_mbps)\n",
    "        if self._predictor is not None:\n",
    "            self._predictor.update(bandwidth_mbps)\n",
    "    \n",
    "    def predict_next(self):\n",
    "        \"\"\"Predict next bandwidth.\"\"\"\n",
    "        if self._predictor is None or not self.trained:\n",
    "            return np.mean(list(self.history)) if len(self.history) > 0 else self.stats['mean']\n",
    "        \n",
    "        return self._predictor.predict_next()\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        if self._predictor is not None:\n",
    "            self._predictor.save(path)\n",
    "        else:\n",
    "            # Save basic stats for compatibility\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'sequence_length': self.sequence_length,\n",
    "                    'alpha': self.alpha,\n",
    "                    'trained': self.trained,\n",
    "                    'stats': self.stats\n",
    "                }, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model.\"\"\"\n",
    "        try:\n",
    "            # Try loading as new LSTM model\n",
    "            self._predictor = LSTMPredictor(sequence_length=self.sequence_length)\n",
    "            self._predictor.load(path)\n",
    "            self.stats = self._predictor.stats.copy()\n",
    "            self.trained = self._predictor.trained\n",
    "            self.sequence_length = self._predictor.sequence_length\n",
    "            self.history = deque(maxlen=self.sequence_length)\n",
    "        except (KeyError, TypeError):\n",
    "            # Fall back to old format\n",
    "            with open(path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.sequence_length = data['sequence_length']\n",
    "                self.alpha = data.get('alpha', 0.3)\n",
    "                self.trained = data['trained']\n",
    "                self.stats = data['stats']\n",
    "                self.history = deque(maxlen=self.sequence_length)\n",
    "                self._predictor = None\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Main Execution\n",
    "\n",
    "Example usage showing how to train the LSTM model:\n",
    "\n",
    "1. Set up paths to bandwidth data and model output\n",
    "2. Call the training pipeline\n",
    "3. Model is automatically saved with tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - configure paths for your environment\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Get project root directory\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        project_root = os.path.dirname(script_dir)\n",
    "    except NameError:\n",
    "        project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    bandwidth_dir = os.path.join(project_root, 'bandwith')\n",
    "    model_dir = os.path.join(project_root, 'models')\n",
    "    output_path = os.path.join(model_dir, 'bandwidth_lstm.pkl')\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(bandwidth_dir):\n",
    "        print(f\"Error: Bandwidth directory not found at {bandwidth_dir}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LSTM Bandwidth Prediction Model Training\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model = train_lstm_model(bandwidth_dir, output_path, tune=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Model trained and saved successfully!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook presents a complete LSTM-based bandwidth prediction system with:\n",
    "\n",
    "### Key Components:\n",
    "1. **BandwidthDataset**: PyTorch dataset for efficient data loading\n",
    "2. **BandwidthLSTM**: Neural network architecture with configurable layers\n",
    "3. **LSTMPredictor**: High-level API for training and inference\n",
    "4. **SimpleLSTM**: Backward-compatible wrapper\n",
    "\n",
    "### Features:\n",
    "- ✅ Train/test split for proper validation\n",
    "- ✅ Early stopping to prevent overfitting\n",
    "- ✅ Gradient clipping for training stability\n",
    "- ✅ Hyperparameter tuning via grid search\n",
    "- ✅ Model persistence (save/load)\n",
    "- ✅ Real-time prediction support\n",
    "- ✅ Comprehensive metrics (MAE, MAPE, RMSE)\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "# Train a new model\n",
    "predictor = train_lstm_model('bandwidth_dir', 'model.pkl', tune=True)\n",
    "\n",
    "# Load existing model\n",
    "predictor = LSTMPredictor()\n",
    "predictor.load('model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictor.update(current_bandwidth)\n",
    "next_bandwidth = predictor.predict_next()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
